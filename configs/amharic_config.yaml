# Amharic-specific configuration for IndexTTS2 fine-tuning
# OPTIMIZED FOR FULL LAYER TRAINING (Community-Proven Approach)
# Use this for 100-200hr+ datasets with T4 GPU (16GB VRAM)

dataset:
    bpe_model: amharic_bpe.model
    sample_rate: 24000
    squeeze: false
    mel:
        sample_rate: 24000
        n_fft: 1024
        hop_length: 256
        win_length: 1024
        n_mels: 100
        mel_fmin: 0
        normalize: false

gpt:
    model_dim: 1280
    max_mel_tokens: 1815
    max_text_tokens: 600
    heads: 20
    use_mel_codes_as_input: true
    mel_length_compression: 1024
    layers: 24  # ALL 24 LAYERS TRAINED
    number_text_tokens: 8000  # Amharic vocabulary size
    number_mel_codes: 8194
    start_mel_token: 8192
    stop_mel_token: 8193
    start_text_token: 0
    stop_text_token: 1
    train_solo_embeddings: true  # TRAIN ALL EMBEDDINGS for new language
    condition_type: "conformer_perceiver"
    condition_module:
        output_size: 512
        linear_units: 2048
        attention_heads: 8
        num_blocks: 6
        input_layer: "conv2d2"
        perceiver_mult: 2
    emo_condition_module:
        output_size: 512
        linear_units: 1024
        attention_heads: 4
        num_blocks: 4
        input_layer: "conv2d2"
        perceiver_mult: 2

# LoRA DISABLED - Full Layer Training for New Languages
lora:
    enabled: false  # ❌ DISABLED: Community-proven to fail for new languages
    rank: 0
    alpha: 0
    dropout: 0
    target_modules: []
    bias: "none"

# Training configuration - FULL LAYER OPTIMIZED FOR T4 GPU
training:
    num_epochs: 8  # Optimized for full layer training
    batch_size: 1  # REDUCED for T4 memory constraints
    learning_rate: 2e-5  # LOWER LR for full training stability
    warmup_steps: 3000  # MORE warmup for full training
    gradient_clip_val: 0.3  # TIGHTER clipping for stability
    save_every: 250  # MORE FREQUENT saves
    log_every: 25   # MORE FREQUENT logging
    weight_decay: 0.01
    
    # Gradient accumulation for effective larger batch size
    gradient_accumulation_steps: 16  # Effective batch size: 1 * 16 = 16
    max_grad_norm: 0.3
    
    # Anti-overfitting measures (CRITICAL for full training)
    validation_split: 0.15  # Larger validation set
    early_stopping_patience: 4  # Increased patience for full training
    min_epochs_before_early_stop: 4  # Don't stop too early
    
    # Data augmentation for diversity
    augmentation:
        speed_perturbation: true
        speed_range: [0.9, 1.1]  # Subtle speed changes
        pitch_perturbation: true
        pitch_range: [-0.5, 0.5]  # Semitones
        noise_injection: true
        noise_level: 0.005  # Very low noise
        time_stretching: true
        stretch_range: [0.9, 1.1]
        
        # Amharic-specific augmentation
        vowel_length_variation: true
        consonant_clarity_boost: true
    
    # Advanced regularization for full training
    regularization:
        dropout: 0.25  # HIGHER dropout for full training
        label_smoothing: 0.2  # INCREASED smoothing
        layer_dropout: 0.15  # Layer dropout
        attention_dropout: 0.2  # Higher attention dropout
        hidden_dropout: 0.15  # Hidden state dropout
    
    # CRITICAL: Memory optimization for T4 GPU (16GB VRAM)
    memory_optimization:
        mixed_precision: true  # ✅ CRITICAL: FP16 essential
        gradient_checkpointing: true  # ✅ CRITICAL: Save VRAM
        activation_checkpointing: true  # ✅ CRITICAL: Checkpoint activations
        low_memory_mode: true  # Additional memory saving
        cpu_offload: true  # Offload to CPU when possible

# Amharic-specific text processing (PRESERVED)
amharic_text:
    preserve_script: true  # Keep modern Amharic script (ፊደል)
    normalize_unicode: true
    expand_contractions: true
    expand_abbreviations: true
    normalize_numbers: true
    normalize_punctuation: true
    
    # Script-specific handling
    handle_syllabic_structure: true
    preserve_vowel_markers: true
    maintain_consonant_clusters: true

# Amharic phonetics configuration (PRESERVED)
amharic_phonetics:
    vowels: ["አ", "ኡ", "ኢ", "ኦ", "ኤ", "ኣ", "እ"]
    consonants: ["ሀ", "ለ", "መ", "ሠ", "ተ", "ቨ", "ነ", "ፐ"]
    syllabic_structure: "CV"
    stress_pattern: "penultimate"
    tone_system: "non-tonal"
    ejective_consonants: ["ጵ", "ጥ", "ፍ", "ት", "ቅ"]
    geminate_consonants: true

# Evaluation configuration (PRESERVED)
evaluation:
    metrics:
        - "cer"
        - "wer"
        - "mos"
        - "speaker_similarity"
        - "emotion_accuracy"
        - "duration_accuracy"
        - "amharic_phoneme_accuracy"
        - "syllable_rhythm_accuracy"
    
    test_sets:
        - "in_domain"
        - "out_of_domain"
        - "emotional"
        - "long_utterances"
        - "formal_amharic"
        - "conversational_amharic"
    
    amharic_evaluation:
        script_preservation_accuracy: true
        vowel_length_accuracy: true
        consonant_cluster_accuracy: true
        stress_pattern_accuracy: true
        rhythm_accuracy: true
        intonation_accuracy: true

# Model paths (PRESERVED)
model_paths:
    pretrained_model: "checkpoints/gpt.pth"
    amharic_vocab: "amharic_bpe.model"
    speaker_encoder: "checkpoints/spk_matrix.pt"
    emotion_encoder: "checkpoints/emo_matrix.pt"
    vocoder: "nvidia/bigvgan_v2_22khz_80band_256x"
    amharic_tokenizer: "amharic_bpe.model"

# Output configuration - FULL TRAINING
output:
    checkpoint_dir: "checkpoints/amharic_full_training"
    log_dir: "logs/amharic_full_training"
    sample_dir: "samples/amharic_full_training"
    tensorboard_dir: "runs/amharic_full_training"
    
    save_best: true
    save_last: true
    save_every_n_epochs: 1
    save_top_k: 5  # Keep top 5 checkpoints
    
    # Sample generation (for monitoring)
    generate_samples_every: 125  # MORE FREQUENT sampling
    num_samples: 3
    sample_texts:
        - "ሰላም ዓለም! ይህ የአማርኛ ሙከራ ነው።"
        - "መልካም ቀን! እንደምን አደርኩልዎት?"
        - "ይህ ዘመቻ ለአማርኛ TTS ስልጠና ነው።"

# Hardware configuration - T4 GPU OPTIMIZED
hardware:
    device: "cuda"
    num_workers: 1  # REDUCED for memory
    pin_memory: true
    mixed_precision: true  # ✅ MUST be true
    gradient_checkpointing: true  # ✅ CRITICAL
    
    # T4 GPU specific optimizations
    memory_efficient_attention: true
    activation_checkpointing: true
    gradient_accumulation_steps: 16  # Effective batch = 16
    
    # CPU utilization
    dataloader_num_workers: 1
    dataloader_pin_memory: true
    cpu_offload: true

# Logging configuration
logging:
    level: "INFO"
    log_to_file: true
    log_to_console: true
    log_to_wandb: true  # CRITICAL for monitoring
    
    wandb:
        project: "indextts2-amharic-full-training"
        entity: null
        tags: ["amharic", "tts", "full-training", "no-lora", "t4-gpu", "production"]
        notes: "Full layer training for Amharic (community-proven approach)"
        log_freq: 25
        save_freq: 250

# Data configuration
data:
    train_split: 0.85  # 85% training
    val_split: 0.15    # 15% validation
    
    # Data filtering
    min_duration: 0.5
    max_duration: 15.0
    min_text_length: 3
    max_text_length: 800
    
    # Data augmentation probability
    augmentation_prob: 0.8  # HIGH for full training
    speed_range: [0.9, 1.1]
    pitch_range: [-1.0, 1.0]
    noise_level: 0.01
    
    # Quality filtering
    quality_filtering:
        min_snr: 10
        max_clipped_ratio: 0.01
        enable_quality_filtering: true

# Anti-overfitting monitoring - ENHANCED FOR FULL TRAINING
monitoring:
    overfitting_indicators:
        - "train_loss_vs_val_loss"
        - "validation_cer_trend"
        - "speaker_similarity_drift"
        - "emotion_accuracy_degradation"
        - "per_layer_loss_distribution"  # Monitor all layers
    
    intervention_triggers:
        val_loss_increase_streak: 3  # Increased patience
        train_val_loss_gap_threshold: 0.15
        validation_cer_increase: 0.05
        layer_saturation_indicator: true
    
    checkpoint_selection:
        primary_metric: "validation_cer"
        secondary_metric: "validation_loss"
        selection_strategy: "best_primary_then_secondary"

# Full training specific settings
full_training_settings:
    enabled: true  # ✅ FULL TRAINING ENABLED
    train_all_layers: true
    train_embeddings: true
    train_layernorms: true
    train_positional_embeddings: true
    memory_optimization_level: "maximum"  # For T4 GPU
    gradient_checkpointing: true
    activation_checkpointing: true
    cpu_offload: true
    offload_optimizer: true
