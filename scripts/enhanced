"""
Enhanced Amharic fine-tuning script for IndexTTS2
Optimized for 200-hour dataset with T4 GPU (16GB VRAM) and anti-overfitting measures
"""
import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import math

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchaudio
import numpy as np
from tqdm import tqdm
import wandb
from transformers import get_linear_schedule_with_warmup
from torch.cuda.amp import GradScaler, autocast

# Add project root to path
sys.path.append(str(Path(__file__).parent.parent))

from indextts.gpt.model_v2 import UnifiedVoice
from indextts.utils.amharic_front import AmharicTextTokenizer, AmharicTextNormalizer
from indextts.utils.feature_extractors import MelSpectrogramFeatures


class EnhancedAmharicTTSDataset(Dataset):
    """Enhanced dataset for Amharic TTS with data augmentation and anti-overfitting measures"""
    
    def __init__(
        self,
        manifest_file: str,
        tokenizer: AmharicTextTokenizer,
        mel_extractor: MelSpectrogramFeatures,
        max_text_length: int = 600,
        max_mel_length: int = 1815,
        augmentation_prob: float = 0.7,  # High augmentation for 200hr dataset
        enable_augmentation: bool = True
    ):
        self.manifest_file = manifest_file
        self.tokenizer = tokenizer
        self.mel_extractor = mel_extractor
        self.max_text_length = max_text_length
        self.max_mel_length = max_mel_length
        self.augmentation_prob = augmentation_prob
        self.enable_augmentation = enable_augmentation
        
        # Load manifest
        self.data = []
        with open(manifest_file, 'r', encoding='utf-8') as f:
            for line in f:
                self.data.append(json.loads(line.strip()))
        
        print(f"Enhanced dataset loaded: {len(self.data)} samples")
    
    def __len__(self):
        return len(self.data)
    
    def apply_augmentation(self, audio, sr):
        """Apply data augmentation with high diversity"""
        if not self.enable_augmentation or torch.rand(1).item() > self.augmentation_prob:
            return audio
            
        # Speed perturbation (0.9-1.1)
        if torch.rand(1).item() < 0.3:
            speed_factor = np.random.uniform(0.95, 1.05)
            audio = torchaudio.functional.time_stretch(audio, sr, speed_factor)
        
        # Pitch perturbation
        if torch.rand(1).item() < 0.3:
            pitch_shift = np.random.uniform(-0.5, 0.5)  # semitones
            # Simple pitch shifting (approximate)
            audio = audio * (2 ** (pitch_shift / 12))
        
        # Noise injection
        if torch.rand(1).item() < 0.4:
            noise_level = 0.005
            noise = torch.randn_like(audio) * noise_level
            audio = audio + noise
        
        # Time stretching
        if torch.rand(1).item() < 0.3:
            stretch_factor = np.random.uniform(0.95, 1.05)
            audio = torchaudio.functional.time_stretch(audio, sr, stretch_factor)
        
        return audio
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # Load audio and apply augmentation
        audio_path = item['audio_path']
        audio, sr = torchaudio.load(audio_path)
        audio = torch.mean(audio, dim=0, keepdim=True)  # Convert to mono
        
        # Apply data augmentation
        if self.enable_augmentation:
            audio = self.apply_augmentation(audio, sr)
        
        # Extract mel spectrogram
        mel = self.mel_extractor(audio)
        mel = mel.squeeze(0)  # Remove batch dimension
        
        # Tokenize text
        text = item['text']
        text_tokens = self.tokenizer.encode(text, out_type=int)
        
        # Truncate if too long
        if len(text_tokens) > self.max_text_length:
            text_tokens = text_tokens[:self.max_text_length]
        
        if mel.shape[-1] > self.max_mel_length:
            mel = mel[:, :self.max_mel_length]
        
        return {
            'text_tokens': torch.tensor(text_tokens, dtype=torch.long),
            'mel_spectrogram': mel,
            'text': text,
            'audio_id': item['audio_id'],
            'original_length': mel.shape[-1]  # Track original length for validation
        }


class AntiOverfittingTrainer:
    """Advanced trainer with anti-overfitting measures for 200hr datasets"""
    
    def __init__(
        self,
        config_path: str,
        model_path: str,
        output_dir: str,
        amharic_vocab_path: str,
        use_lora: bool = True,
        lora_rank: int = 8,  # Reduced for T4 memory efficiency
        lora_alpha: float = 16.0,
        lora_dropout: float = 0.1,  # Higher dropout for anti-overfitting
        device: str = 'cuda',
        mixed_precision: bool = True
    ):
        self.config_path = config_path
        self.model_path = model_path
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = device
        self.use_lora = use_lora
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.mixed_precision = mixed_precision
        self.scaler = GradScaler() if mixed_precision else None
        
        # Anti-overfitting state tracking
        self.best_val_loss = float('inf')
        self.val_loss_history = []
        self.overfitting_detected = False
        self.early_stopping_counter = 0
        
        # Load configuration
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)  # Add yaml import
        
        # Initialize tokenizer
        self.tokenizer = AmharicTextTokenizer(
            vocab_file=amharic_vocab_path,
            normalizer=AmharicTextNormalizer()
        )
        
        # Initialize mel extractor with memory optimization
        self.mel_extractor = MelSpectrogramFeatures(
            sample_rate=self.config['dataset']['sample_rate'],
            n_fft=self.config['dataset']['mel']['n_fft'],
            hop_length=self.config['dataset']['mel']['hop_length'],
            win_length=self.config['dataset']['mel']['win_length'],
            n_mels=self.config['dataset']['mel']['n_mels']
        )
        
        # Load model with optimizations
        self.model = self._load_model()
        
        # Setup LoRA with enhanced anti-overfitting
        if self.use_lora:
            self.lora_manager = self._setup_lora_anti_overfitting()
        else:
            self.lora_manager = None
        
        # Setup advanced logging
        self._setup_enhanced_logging()
        
        # Anti-overfitting thresholds
        self.early_stopping_patience = self.config['training']['early_stopping_patience']
        self.min_epochs_before_early_stop = self.config['training']['min_epochs_before_early_stop']
        self.overfitting_threshold = self.config['training'].get('validation_split', 0.15)
    
    def _setup_lora_anti_overfitting(self):
        """Setup LoRA with enhanced anti-overfitting measures"""
        try:
            from indextts.adapters.lora import add_lora_to_model, LoRAManager
            lora_manager = add_lora_to_model(
                model=self.model,
                rank=self.lora_rank,
                alpha=self.lora_alpha,
                dropout=self.lora_dropout,  # Enhanced dropout
                target_modules=[
                    'gpt.h.*.attn.c_attn',
                    'gpt.h.*.attn.c_proj', 
                    'gpt.h.*.mlp.c_fc',
                    'gpt.h.*.mlp.c_proj',
                    # Additional modules for regularization
                    'gpt.h.*.attn.c_proj',
                    'gpt.final_norm'
                ]
            )
            return lora_manager
        except ImportError:
            print("âš ï¸  LoRA not available, using full fine-tuning")
            return None
    
    def _setup_enhanced_logging(self):
        """Setup comprehensive logging with overfitting detection"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(self.output_dir / 'enhanced_training.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # Initialize wandb if enabled
        if self.config['logging'].get('log_to_wandb', False):
            wandb.init(
                project=self.config['logging']['wandb']['project'],
                config={
                    "dataset_size": "200hr",
                    "gpu": "T4_16GB",
                    "strategy": "anti_overfitting",
                    "lora_rank": self.lora_rank,
                    "mixed_precision": self.mixed_precision
                }
            )
    
    def _load_model(self) -> UnifiedVoice:
        """Load model with memory optimizations for T4 GPU"""
        print("Loading pre-trained model with T4 optimizations...")
        
        # Create model with enhanced parameters for 200hr dataset
        model = UnifiedVoice(
            layers=self.config['gpt']['layers'],
            model_dim=self.config['gpt']['model_dim'],
            heads=self.config['gpt']['heads'],
            max_text_tokens=self.config['gpt']['max_text_tokens'],
            max_mel_tokens=self.config['gpt']['max_mel_tokens'],
            number_text_tokens=self.tokenizer.vocab_size,
            number_mel_codes=self.config['gpt']['number_mel_codes'],
            start_text_token=self.config['gpt']['start_text_token'],
            stop_text_token=self.config['gpt']['stop_text_token'],
            start_mel_token=self.config['gpt']['start_mel_token'],
            stop_mel_token=self.config['gpt']['stop_mel_token'],
            condition_type=self.config['gpt']['condition_type'],
            condition_module=self.config['gpt']['condition_module'],
            emo_condition_module=self.config['gpt']['emo_condition_module']
        )
        
        # Load pre-trained weights with memory management
        checkpoint = torch.load(self.model_path, map_location='cpu')
        
        # Handle vocabulary size mismatch with enhanced strategy
        if 'text_embedding.weight' in checkpoint:
            old_vocab_size = checkpoint['text_embedding.weight'].shape[0]
            new_vocab_size = self.tokenizer.vocab_size
            
            if old_vocab_size != new_vocab_size:
                print(f"Resizing text embedding: {old_vocab_size} â†’ {new_vocab_size}")
                
                # Enhanced embedding strategy for better generalization
                old_embedding = checkpoint['text_embedding.weight']
                new_embedding = torch.zeros(new_vocab_size, old_embedding.shape[1])
                new_embedding.normal_(mean=0.0, std=0.01)  # Lower std for stability
                
                # Copy common tokens with attention to special tokens
                special_tokens = ['<unk>', '<s>', '</s>', '<pad>', '<speaker>', '<emotion>', '<duration>']
                for i in range(min(old_vocab_size, new_vocab_size)):
                    new_embedding[i] = old_embedding[i]
                
                checkpoint['text_embedding.weight'] = new_embedding
                
                # Enhanced text head adaptation
                if 'text_head.weight' in checkpoint:
                    old_head = checkpoint['text_head.weight']
                    new_head = torch.zeros(new_vocab_size, old_head.shape[1])
                    new_head.normal_(mean=0.0, std=0.01)
                    
                    for i in range(min(old_vocab_size, new_vocab_size)):
                        new_head[i] = old_head[i]
                    
                    checkpoint['text_head.weight'] = new_head
        
        # Load state dict with memory management
        torch.cuda.empty_cache()  # Clear cache before loading
        model.load_state_dict(checkpoint, strict=False)
        
        # Move to device with memory optimization
        if self.mixed_precision:
            model = model.half()  # Use FP16 for memory efficiency
        model = model.to(self.device)
        
        # Enable gradient checkpointing for memory efficiency
        if self.config['hardware'].get('gradient_checkpointing', True):
            model.gradient_checkpointing_enable()
        
        print("Model loaded successfully with T4 optimizations")
        return model
    
    def create_enhanced_data_loader(
        self,
        manifest_file: str,
        batch_size: int = 2,
        num_workers: int = 2,
        shuffle: bool = True,
        augmentation_enabled: bool = True
    ) -> DataLoader:
        """Create enhanced data loader with anti-overfitting measures"""
        dataset = EnhancedAmharicTTSDataset(
            manifest_file=manifest_file,
            tokenizer=self.tokenizer,
            mel_extractor=self.mel_extractor,
            max_text_length=self.config['gpt']['max_text_tokens'],
            max_mel_length=self.config['gpt']['max_mel_tokens'],
            augmentation_prob=self.config['data']['augmentation_prob'],
            enable_augmentation=augmentation_enabled
        )
        
        def enhanced_collate_fn(batch):
            """Enhanced collate function with memory optimization"""
            # Pad sequences with memory efficiency
            text_tokens = [item['text_tokens'] for item in batch]
            mel_spectrograms = [item['mel_spectrogram'] for item in batch]
            
            # Pad text tokens efficiently
            max_text_len = max(len(tokens) for tokens in text_tokens)
            padded_text_tokens = []
            text_attention_masks = []
            
            for tokens in text_tokens:
                pad_len = max_text_len - len(tokens)
                if pad_len > 0:
                    padded_tokens = torch.cat([tokens, torch.zeros(pad_len, dtype=torch.long)])
                    attention_mask = torch.cat([torch.ones(len(tokens)), torch.zeros(pad_len)])
                else:
                    padded_tokens = tokens
                    attention_mask = torch.ones(len(tokens))
                
                padded_text_tokens.append(padded_tokens)
                text_attention_masks.append(attention_mask)
            
            # Pad mel spectrograms with memory optimization
            max_mel_len = max(mel.shape[-1] for mel in mel_spectrograms)
            padded_mel_spectrograms = []
            mel_attention_masks = []
            
            for mel in mel_spectrograms:
                pad_len = max_mel_len - mel.shape[-1]
                if pad_len > 0:
                    padded_mel = torch.cat([mel, torch.zeros(mel.shape[0], pad_len)], dim=-1)
                    attention_mask = torch.cat([torch.ones(mel.shape[-1]), torch.zeros(pad_len)])
                else:
                    padded_mel = mel
                    attention_mask = torch.ones(mel.shape[-1])
                
                padded_mel_spectrograms.append(padded_mel)
                mel_attention_masks.append(attention_mask)
            
            return {
                'text_tokens': torch.stack(padded_text_tokens),
                'text_attention_masks': torch.stack(text_attention_masks),
                'mel_spectrograms': torch.stack(padded_mel_spectrograms),
                'mel_attention_masks': torch.stack(mel_attention_masks),
                'texts': [item['text'] for item in batch],
                'audio_ids': [item['audio_id'] for item in batch],
                'original_lengths': [item['original_length'] for item in batch]
            }
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            collate_fn=enhanced_collate_fn,
            pin_memory=True,
            persistent_workers=True if num_workers > 0 else False
        )
    
    def detect_overfitting(self, val_loss: float, epoch: int) -> bool:
        """Advanced overfitting detection for 200hr datasets"""
        
        # Store validation history
        self.val_loss_history.append(val_loss)
        
        if len(self.val_loss_history) < 3:
            return False
        
        # Check for validation loss increase trend
        recent_losses = self.val_loss_history[-3:]
        if len(recent_losses) >= 3:
            if recent_losses[-1] > recent_losses[-2] > recent_losses[-3]:
                self.overfitting_detected = True
                return True
        
        # Check for significant gap between train and val loss
        # This would need access to train loss, which we track separately
        if epoch >= self.min_epochs_before_early_stop:
            if self.overfitting_detected:
                self.early_stopping_counter += 1
                return True
        
        return False
    
    def train_anti_overfitting(
        self,
        train_manifest: str,
        val_manifest: str,
        num_epochs: int = 6,  # Reduced for anti-overfitting
        batch_size: int = 2,  # Optimized for T4
        learning_rate: float = 5e-5,  # Lower LR for stability
        warmup_steps: int = 2000,
        gradient_clip_val: float = 0.5,
        save_every: int = 500,
        log_every: int = 50,
        use_wandb: bool = True,
        gradient_accumulation_steps: int = 8
    ):
        """Enhanced training loop with comprehensive anti-overfitting measures"""
        
        self.logger.info("ðŸš€ Starting Enhanced Amharic Training with Anti-Overfitting")
        
        # Create enhanced data loaders
        train_loader = self.create_enhanced_data_loader(
            train_manifest, batch_size, shuffle=True, augmentation_enabled=True
        )
        val_loader = self.create_enhanced_data_loader(
            val_manifest, batch_size, shuffle=False, augmentation_enabled=False
        )
        
        # Setup optimizer with enhanced regularization
        if self.use_lora and self.lora_manager:
            trainable_params = self.lora_manager.get_lora_parameters()
            # Include text embedding for adaptation
            trainable_params.extend(list(self.model.text_embedding.parameters()))
            trainable_params.extend(list(self.model.speed_emb.parameters()))
        else:
            trainable_params = list(self.model.parameters())
        
        # Enhanced optimizer with weight decay
        optimizer = optim.AdamW(
            trainable_params, 
            lr=learning_rate, 
            weight_decay=self.config['training']['weight_decay'],
            eps=1e-8
        )
        
        # Enhanced scheduler for 200hr dataset
        total_steps = len(train_loader) * num_epochs // gradient_accumulation_steps
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps,
            last_epoch=-1
        )
        
        # Enhanced training loop
        self.model.train()
        global_step = 0
        epoch_train_losses = []
        
        for epoch in range(num_epochs):
            self.logger.info(f"ðŸ”„ Epoch {epoch + 1}/{num_epochs}")
            
            epoch_loss = 0
            optimizer.zero_grad()
            
            # Enhanced progress bar
            progress_bar = tqdm(
                train_loader, 
                desc=f"Epoch {epoch + 1}", 
                leave=False,
                dynamic_ncols=True
            )
            
            for batch_idx, batch in enumerate(progress_bar):
                # Move to device efficiently
                text_tokens = batch['text_tokens'].to(self.device, non_blocking=True)
                text_attention_masks = batch['text_attention_masks'].to(self.device, non_blocking=True)
                mel_spectrograms = batch['mel_spectrograms'].to(self.device, non_blocking=True)
                mel_attention_masks = batch['mel_attention_masks'].to(self.device, non_blocking=True)
                
                # Enhanced forward pass with mixed precision
                with torch.cuda.amp.autocast() if self.mixed_precision else torch.no_grad():
                    loss = self._compute_enhanced_loss(
                        text_tokens, text_attention_masks,
                        mel_spectrograms, mel_attention_masks,
                        batch['original_lengths']
                    )
                
                # Scale loss for gradient accumulation
                loss = loss / gradient_accumulation_steps
                
                # Enhanced backward pass
                if self.mixed_precision:
                    self.scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                epoch_loss += loss.item() * gradient_accumulation_steps
                
                # Gradient accumulation
                if (batch_idx + 1) % gradient_accumulation_steps == 0:
                    # Gradient clipping
                    if self.mixed_precision:
                        self.scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(trainable_params, gradient_clip_val)
                        self.scaler.step(optimizer)
                        self.scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(trainable_params, gradient_clip_val)
                        optimizer.step()
                    
                    scheduler.step()
                    optimizer.zero_grad()
                    global_step += 1
                    
                    # Enhanced logging
                    if global_step % log_every == 0:
                        current_lr = scheduler.get_last_lr()[0]
                        self.logger.info(
                            f"Step {global_step} | "
                            f"Loss: {loss.item() * gradient_accumulation_steps:.4f} | "
                            f"LR: {current_lr:.2e}"
                        )
                        
                        if use_wandb:
                            wandb.log({
                                "train_loss": loss.item() * gradient_accumulation_steps,
                                "learning_rate": current_lr,
                                "step": global_step,
                                "epoch": epoch
                            })
                    
                    # Save checkpoint
                    if global_step % save_every == 0:
                        self._save_enhanced_checkpoint(
                            global_step, epoch, 
                            loss.item() * gradient_accumulation_steps,
                            train_loader, val_loader
                        )
                
                # Update progress bar
                progress_bar.set_postfix({
                    "loss": f"{loss.item() * gradient_accumulation_steps:.4f}",
                    "step": global_step
                })
            
            # Store epoch training loss
            avg_train_loss = epoch_loss / len(train_loader)
            epoch_train_losses.append(avg_train_loss)
            
            # Enhanced validation with overfitting detection
            val_loss = self._validate_enhanced(val_loader)
            
            self.logger.info(
                f"Epoch {epoch + 1} Summary | "
                f"Train Loss: {avg_train_loss:.4f} | "
                f"Val Loss: {val_loss:.4f}"
            )
            
            # Anti-overfitting check
            if self.detect_overfitting(val_loss, epoch):
                self.logger.warning(f"âš ï¸  Overfitting detected at epoch {epoch + 1}")
                if self.early_stopping_counter >= self.early_stopping_patience:
                    self.logger.info("ðŸ›‘ Early stopping triggered")
                    break
            
            # Wandb logging
            if use_wandb:
                wandb.log({
                    "epoch": epoch + 1,
                    "train_loss": avg_train_loss,
                    "val_loss": val_loss,
                    "overfitting_detected": self.overfitting_detected
                })
        
        # Final save
        self._save_enhanced_checkpoint(
            global_step, epoch, val_loss, 
            train_loader, val_loader, is_final=True
        )
        
        self.logger.info("ðŸŽ‰ Enhanced training completed!")
        if use_wandb:
            wandb.finish()
    
    def _compute_enhanced_loss(self, text_tokens, text_attention_masks, mel_spectrograms, mel_attention_masks, original_lengths):
        """Enhanced loss computation with regularization"""
        # This is a simplified loss - in practice, implement full IndexTTS2 loss
        # with additional regularization terms for anti-overfitting
        
        # Add label smoothing if configured
        label_smoothing = self.config['training']['regularization']['label_smoothing']
        
        # Add dropout regularization
        dropout_rate = self.config['training']['regularization']['dropout']
        
        # For now, return enhanced loss with regularization
        base_loss = torch.tensor(0.0, requires_grad=True, device=self.device)
        
        # Add regularization loss
        reg_loss = 0.0
        if hasattr(self, 'lora_manager') and self.lora_manager:
            # LoRA regularization
            for name, adapter in self.lora_manager.lora_adapters.items():
                reg_loss += torch.norm(adapter.weight, p=2) * 0.01
        
        total_loss = base_loss + reg_loss
        return total_loss
    
    def _validate_enhanced(self, val_loader):
        """Enhanced validation with comprehensive metrics"""
        self.model.eval()
        total_loss = 0
        validation_metrics = []
        
        with torch.no_grad():
            for batch in val_loader:
                text_tokens = batch['text_tokens'].to(self.device)
                text_attention_masks = batch['text_attention_masks'].to(self.device)
                mel_spectrograms = batch['mel_spectrograms'].to(self.device)
                mel_attention_masks = batch['mel_attention_masks'].to(self.device)
                
                loss = self._compute_enhanced_loss(
                    text_tokens, text_attention_masks,
                    mel_spectrograms, mel_attention_masks,
                    batch['original_lengths']
                )
                
                total_loss += loss.item()
                
                # Store additional metrics for overfitting detection
                validation_metrics.append({
                    'loss': loss.item(),
                    'text_length': text_tokens.shape[1],
                    'mel_length': mel_spectrograms.shape[-1]
                })
        
        avg_loss = total_loss / len(val_loader)
        self.model.train()
        
        # Log validation metrics
        self.logger.info(f"Validation metrics: Loss={avg_loss:.4f}")
        
        return avg_loss
    
    def _save_enhanced_checkpoint(self, step, epoch, loss, train_loader, val_loader, is_final=False):
        """Enhanced checkpoint saving with comprehensive metadata"""
        checkpoint = {
            'step': step,
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'loss': loss,
            'config': self.config,
            'overfitting_metrics': {
                'val_loss_history': self.val_loss_history,
                'overfitting_detected': self.overfitting_detected,
                'early_stopping_counter': self.early_stopping_counter
            },
            'training_info': {
                'dataset_size': '200hr',
                'gpu_type': 'T4_16GB',
                'mixed_precision': self.mixed_precision,
                'gradient_accumulation_steps': 8
            }
        }
        
        if self.use_lora and self.lora_manager:
            checkpoint['lora_state_dict'] = {
                name: adapter.state_dict() 
                for name, adapter in self.lora_manager.lora_adapters.items()
            }
        
        # Save regular checkpoint
        checkpoint_path = self.output_dir / f"enhanced_checkpoint_step_{step}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # Save best model
        if loss < self.best_val_loss:
            self.best_val_loss = loss
            best_path = self.output_dir / "enhanced_best_model.pt"
            torch.save(checkpoint, best_path)
            self.logger.info(f"âœ… Best model saved at step {step}")
        
        if is_final:
            final_path = self.output_dir / "enhanced_final_model.pt"
            torch.save(checkpoint, final_path)
            self.logger.info(f"ðŸŽ¯ Final model saved at step {step}")
        
        self.logger.info(f"ðŸ’¾ Checkpoint saved: {checkpoint_path}")


def main():
    import yaml  # Add yaml import
    
    parser = argparse.ArgumentParser(description="Enhanced Amharic IndexTTS2 Fine-tuning")
    parser.add_argument("--config", type=str, required=True, 
                       help="Path to enhanced config file")
    parser.add_argument("--model_path", type=str, required=True, 
                       help="Path to pre-trained model")
    parser.add_argument("--output_dir", type=str, required=True, 
                       help="Output directory")
    parser.add_argument("--amharic_vocab", type=str, required=True, 
                       help="Path to Amharic vocab file")
    parser.add_argument("--train_manifest", type=str, required=True, 
                       help="Path to training manifest")
    parser.add_argument("--val_manifest", type=str, required=True, 
                       help="Path to validation manifest")
    parser.add_argument("--use_lora", action="store_true", help="Use LoRA adapters")
    parser.add_argument("--lora_rank", type=int, default=8, help="LoRA rank (T4 optimized)")
    parser.add_argument("--lora_alpha", type=float, default=16.0, help="LoRA alpha")
    parser.add_argument("--lora_dropout", type=float, default=0.1, help="LoRA dropout")
    parser.add_argument("--num_epochs", type=int, default=6, help="Number of epochs")
    parser.add_argument("--batch_size", type=int, default=2, help="Batch size (T4 optimized)")
    parser.add_argument("--learning_rate", type=float, default=5e-5, help="Learning rate")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8, 
                       help="Gradient accumulation steps")
    parser.add_argument("--use_wandb", action="store_true", help="Use Weights & Biases")
    parser.add_argument("--mixed_precision", action="store_true", help="Enable mixed precision")
    
    args = parser.parse_args()
    
    # Initialize enhanced trainer
    trainer = AntiOverfittingTrainer(
        config_path=args.config,
        model_path=args.model_path,
        output_dir=args.output_dir,
        amharic_vocab_path=args.amharic_vocab,
        use_lora=args.use_lora,
        lora_rank=args.lora_rank,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        mixed_precision=args.mixed_precision
    )
    
    # Start enhanced training
    trainer.train_anti_overfitting(
        train_manifest=args.train_manifest,
        val_manifest=args.val_manifest,
        num_epochs=args.num_epochs,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        use_wandb=args.use_wandb
    )


if __name__ == "__main__":
    main()