# Amharic IndexTTS2 Fine-tuning System

Complete adaptation of IndexTTS2 for **Modern Amharic (áŠá‹°áˆ)** language using LoRA-based efficient fine-tuning.

## ğŸŒ Overview

This system provides a comprehensive pipeline for fine-tuning the IndexTTS2 model to generate high-quality speech synthesis for Amharic language, specifically designed for the modern Amharic script (áŠá‹°áˆ). It includes specialized text processing, tokenization, vocabulary training, dataset preparation, model fine-tuning, and evaluation components.

### Key Features

- âœ… **Complete Amharic Language Support** - Modern Amharic script (áŠá‹°áˆ) handling
- âœ… **Efficient LoRA Fine-tuning** - Parameter-efficient adaptation using LoRA
- âœ… **Specialized Text Processing** - Amharic-specific normalization and tokenization
- âœ… **Optimized Vocabulary** - SentencePiece BPE model trained on Amharic text
- âœ… **Comprehensive Evaluation** - Multi-dimensional quality assessment
- âœ… **Production-Ready Pipeline** - End-to-end automation from data to inference

## ğŸ“‹ Prerequisites

### System Requirements
- Python 3.8+
- CUDA 11.0+ (for GPU training)
- 16GB+ RAM
- 100GB+ storage space

### Dependencies
```bash
pip install torch torchaudio
pip install sentencepiece
pip install transformers
pip install pyyaml
pip install tqdm
pip install numpy scipy
pip install jiwer  # For evaluation metrics
pip install wandb  # Optional, for experiment tracking
pip install peft   # For LoRA (optional, manual LoRA available)
```

### IndexTTS2 Setup
Ensure you have the base IndexTTS2 model and dependencies installed. Place the pre-trained model checkpoint at `checkpoints/gpt.pth`.

## ğŸš€ Quick Start

### 1. Prepare Your Data
Organize your Amharic dataset with matching audio-text pairs:

```
amharic_dataset/
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ sample_001.wav
â”‚   â”œâ”€â”€ sample_002.wav
â”‚   â””â”€â”€ ...
â””â”€â”€ text/
    â”œâ”€â”€ sample_001.txt
    â”œâ”€â”€ sample_002.txt
    â””â”€â”€ ...
```

### 2. Run Complete Training Pipeline
```bash
chmod +x scripts/run_amharic_training.sh
./scripts/run_amharic_training.sh \
    --audio-dir ./amharic_dataset/audio \
    --text-dir ./amharic_dataset/text \
    --pretrained-model ./checkpoints/gpt.pth \
    --batch-size 4 \
    --epochs 15 \
    --wandb
```

### 3. Quick Individual Steps

#### Step 1: Dataset Preparation
```bash
python scripts/prepare_amharic_data.py \
    --audio_dir ./amharic_dataset/audio \
    --text_dir ./amharic_dataset/text \
    --output_dir ./amharic_dataset/prepared
```

#### Step 2: Vocabulary Training
```bash
python scripts/train_amharic_vocabulary.py \
    --text_files ./amharic_dataset/text/*.txt \
    --output_dir ./amharic_models \
    --model_prefix amharic_bpe
```

#### Step 3: Model Fine-tuning
```bash
python scripts/finetune_amharic.py \
    --config configs/amharic_config.yaml \
    --model_path ./checkpoints/gpt.pth \
    --output_dir ./checkpoints/amharic \
    --amharic_vocab ./amharic_models/amharic_bpe.model \
    --train_manifest ./amharic_dataset/prepared/train_manifest.jsonl \
    --val_manifest ./amharic_dataset/prepared/val_manifest.jsonl \
    --use_lora \
    --num_epochs 15
```

#### Step 4: Model Evaluation
```bash
python scripts/evaluate_amharic.py \
    --config configs/amharic_config.yaml \
    --model_path ./checkpoints/amharic/best_amharic_model.pt \
    --amharic_vocab ./amharic_models/amharic_bpe.model \
    --output_dir ./checkpoints/amharic/evaluation
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ amharic_config.yaml          # Amharic-specific configuration
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_amharic_data.py      # Dataset preparation script
â”‚   â”œâ”€â”€ train_amharic_vocabulary.py  # Vocabulary training script
â”‚   â”œâ”€â”€ finetune_amharic.py         # Main fine-tuning script
â”‚   â”œâ”€â”€ evaluate_amharic.py         # Evaluation script
â”‚   â””â”€â”€ run_amharic_training.sh     # Complete pipeline automation
â”œâ”€â”€ indextts/utils/
â”‚   â””â”€â”€ amharic_front.py             # Amharic text processing module
â”œâ”€â”€ amharic_models/                  # Trained vocabulary models
â”œâ”€â”€ amharic_dataset/                # Prepared dataset manifests
â””â”€â”€ checkpoints/amharic/            # Fine-tuned model checkpoints
```

## ğŸ”§ Core Components

### 1. Amharic Text Processing (`amharic_front.py`)

**Features:**
- Modern Amharic script (áŠá‹°áˆ) preservation
- Number word normalization (áŠ áŠ•á‹µ â†’ 1, áˆáˆˆá‰µ â†’ 2, etc.)
- Abbreviation expansion (á‹¶/áˆ­ â†’ á‹¶áŠ­á‰°áˆ­)
- Contraction handling (áŠ¨áˆ†áŠ â†’ áŠ¨ áˆ†áŠ)
- Unicode normalization for Amharic

**Usage:**
```python
from indextts.utils.amharic_front import AmharicTextNormalizer

normalizer = AmharicTextNormalizer()
normalized = normalizer.normalize("áˆ°áˆ‹áˆ! áŠ¥áŠ•á‹°áˆáŠ• áŠ á‹°áˆ­áŠ©? 1+1=2 áŠá‹á¢")
print(normalized)  # "áˆ°áˆ‹áˆ! áŠ¥áŠ•á‹°áˆáŠ• áŠ á‹°áˆ­áŠ©? 1+1=2 áŠá‹á¢"
```

### 2. Amharic Dataset Preparation (`prepare_amharic_data.py`)

**Features:**
- Automatic audio-text pairing discovery
- Multi-format support (WAV, MP3, FLAC for audio; TXT, JSON, LRC for text)
- Quality validation (duration, volume, clipping detection)
- Train/val/test splitting with configurable ratios
- Comprehensive statistics generation

**Supported Formats:**
- **Audio:** `.wav`, `.flac`, `.m4a`, `.mp3`, `.ogg`
- **Text:** `.txt`, `.json`, `.lrc`

### 3. Amharic Vocabulary Training (`train_amharic_vocabulary.py`)

**Features:**
- SentencePiece BPE training optimized for Amharic
- Text cleaning and validation for Amharic
- Vocabulary size optimization based on corpus analysis
- Quality evaluation and coverage analysis
- Automatic vocabulary size suggestion

**Configuration:**
```yaml
# Optimized for Amharic characteristics
vocab_size: 8000              # Sufficient for Amharic vocabulary
character_coverage: 0.9999    # High coverage for Amharic script
amharic_char_coverage: true   # Prioritize Amharic characters
```

### 4. Amharic Fine-tuning (`finetune_amharic.py`)

**Features:**
- LoRA-based efficient fine-tuning
- Amharic-specific data augmentation
- Gradient accumulation for larger effective batch size
- Mixed precision training for memory efficiency
- Automatic checkpointing and best model selection

**LoRA Configuration:**
```yaml
lora:
    enabled: true
    rank: 16                  # Good balance of efficiency and performance
    alpha: 16.0
    dropout: 0.1              # Higher for Amharic to prevent overfitting
    target_modules:
        - "gpt.h.*.attn.c_attn"
        - "gpt.h.*.attn.c_proj"
        - "gpt.h.*.mlp.c_fc"
        - "gpt.h.*.mlp.c_proj"
```

### 5. Amharic Evaluation (`evaluate_amharic.py`)

**Features:**
- Multi-dimensional quality assessment
- Text processing evaluation (tokenization, normalization)
- Inference capability testing
- Audio quality analysis (energy, clipping, silence)
- Amharic linguistic feature validation
- Comprehensive report generation

**Evaluation Metrics:**
- Text processing success rate
- Vocabulary coverage analysis
- Inference stability and speed
- Audio quality indicators
- Amharic script preservation
- Linguistic feature handling

## ğŸ“Š Configuration Options

### Training Configuration
```yaml
training:
    num_epochs: 15           # More epochs for Amharic learning
    batch_size: 4
    learning_rate: 5e-5      # Lower for Amharic
    warmup_steps: 1500       # More warmup steps
    gradient_clip_val: 1.0
    
    augmentation:
        speed_perturbation: true
        speed_range: [0.85, 1.15]  # Wider for Amharic speech
        pitch_perturbation: true
        pitch_range: [-0.2, 0.2]   # Wider pitch range
        noise_injection: true
        noise_level: 0.005
```

### Amharic Text Processing
```yaml
amharic_text:
    preserve_script: true    # Keep modern Amharic script
    normalize_unicode: true
    expand_contractions: true
    expand_abbreviations: true
    normalize_numbers: true
    normalize_punctuation: true
```

### Data Filtering
```yaml
data:
    min_duration: 1.5        # Higher minimum for Amharic
    max_duration: 25.0       # Lower maximum for speech patterns
    min_text_length: 8       # Higher minimum for sentences
    max_text_length: 400     # Lower maximum for efficiency
```

## ğŸ¯ Best Practices

### Data Preparation
1. **Audio Quality:** Use clean, noise-free recordings at 24kHz sample rate
2. **Text Quality:** Ensure proper Amharic spelling and punctuation
3. **Text Length:** Keep utterances between 5-25 seconds for optimal training
4. **Diversity:** Include various speaking styles and emotional tones

### Training Tips
1. **Dataset Size:** Start with 10+ hours of speech, scale to 50+ hours for best results
2. **LoRA Rank:** Use rank 16-32 for most applications, increase for larger datasets
3. **Learning Rate:** Start with 5e-5, adjust based on validation loss
4. **Monitoring:** Use Weights & Biases for experiment tracking

### Model Selection
1. **Vocabulary Size:** Start with 8000 tokens, increase if needed
2. **Batch Size:** Use 4-8 for most GPUs, increase if you have more VRAM
3. **Epochs:** Train for 10-20 epochs depending on dataset size

## ğŸ” Troubleshooting

### Common Issues

**1. Out of Memory Errors**
- Reduce batch size to 2 or 1
- Enable gradient checkpointing
- Use mixed precision training
- Increase gradient accumulation steps

**2. Poor Audio Quality**
- Check input audio quality and format
- Verify sample rate consistency (24kHz)
- Review vocoder configuration
- Analyze audio statistics in evaluation

**3. Text Processing Failures**
- Verify Amharic text encoding (UTF-8)
- Check for unsupported characters
- Review normalization rules
- Validate vocabulary coverage

**4. Slow Training**
- Enable mixed precision (fp16)
- Use multiple GPU workers
- Optimize data loading
- Enable gradient checkpointing

### Performance Optimization

**For 8GB GPU:**
```yaml
batch_size: 2
gradient_accumulation_steps: 4
mixed_precision: true
gradient_checkpointing: true
```

**For 16GB+ GPU:**
```yaml
batch_size: 8
gradient_accumulation_steps: 1
mixed_precision: true
```

## ğŸ“ˆ Expected Results

### Training Performance
- **Training Time:** 2-8 hours per epoch (depending on dataset size and hardware)
- **Memory Usage:** 6-12GB VRAM (with mixed precision and gradient checkpointing)
- **Convergence:** 10-15 epochs for reasonable quality, 20+ epochs for high quality

### Model Performance Metrics
- **Text Processing Success:** >95%
- **Vocabulary Coverage:** >85%
- **Inference Success Rate:** >90%
- **Audio Quality:** MOS score >3.5 (subjective)

## ğŸ”¬ Advanced Usage

### Custom Amharic Text Normalization
```python
from indextts.utils.amharic_front import AmharicTextNormalizer

# Create custom normalizer with additional rules
normalizer = AmharicTextNormalizer()
normalizer.abbreviations.update({
    "áŠ­áˆ­.áˆ.": "áŠ­áˆªáˆµá‰µáˆ«áŠ• áˆ˜áˆ­áˆ áŒá‰¥áˆ­",
    "áŠ á‹²áˆµ áŠ á‰ á‰£": "áŠ á‹²áˆµ áŠ á‰ á‰£"
})

# Use in training pipeline
normalized = normalizer.normalize("áŠ¥áŒá‹šáŠ á‰¥áˆ”áˆ­ á‰ áŠ­áˆ­áˆµá‰¶áˆµ áŠ¥áŠ•áŠ³áŠ• á‹°áˆ…áŠ“ áˆ˜áŒ£áˆ…")
```

### Custom Evaluation Metrics
```python
from scripts.evaluate_amharic import AmharicTTSEvaluator

# Extend evaluator for custom metrics
evaluator = AmharicTTSEvaluator(
    config_path="configs/amharic_config.yaml",
    model_path="checkpoints/amharic/best_amharic_model.pt",
    amharic_vocab_path="amharic_models/amharic_bpe.model"
)

# Add custom evaluation
results = evaluator.run_comprehensive_evaluation(
    test_texts=custom_test_cases,
    test_audio_dir="generated_samples/"
)
```

### Batch Inference
```python
import torch
from indextts.utils.amharic_front import AmharicTextTokenizer

# Load fine-tuned model and tokenizer
tokenizer = AmharicTextTokenizer("amharic_models/amharic_bpe.model")

# Batch inference
texts = [
    "áˆ°áˆ‹áˆ á‹“áˆˆáˆ!",
    "á‹›áˆ¬ á‹¨á‰°áˆ»áˆˆ á‰€áŠ• áŠá‹á¢",
    "áŠ¥á‰£áŠ®á‰µáˆá‰µáŠ• áˆ›áˆ³áˆ˜áŠ• áŠ áˆá‰½áˆáˆá¢"
]

# Tokenize batch
batch_tokens = tokenizer.batch_encode(texts, out_type=int)

# Generate speech (requires actual IndexTTS2 inference implementation)
# generated_audio = model.generate(batch_tokens, ...)
```

## ğŸ“š References

- [IndexTTS2 Original Paper](https://arxiv.org/abs/2306.07304)
- [SentencePiece Documentation](https://github.com/google/sentencepiece)
- [LoRA: Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)
- [Amharic Language Resources](https://en.wikipedia.org/wiki/Amharic)

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch for your Amharic improvements
3. Submit a pull request with detailed description
4. Ensure all tests pass and documentation is updated

## ğŸ“„ License

This project maintains the same license as the original IndexTTS2 repository.

## ğŸ†˜ Support

For issues, questions, or contributions:
1. Check the troubleshooting section above
2. Review existing GitHub issues
3. Create a new issue with detailed information
4. Include model configuration and error logs

---

**Built with â¤ï¸ for the Amharic-speaking community** ğŸ‡ªğŸ‡¹

*This adaptation makes IndexTTS2 accessible to Amharic speakers while preserving the model's powerful capabilities for high-quality speech synthesis.*